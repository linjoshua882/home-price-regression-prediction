{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing and Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data has been cleaned, most features have been selected, and our outliers have been removed. Most of the variables, if not all of the variables, have some relatively strong relationship with sale price. Now we can begin identifying a model that will perform up to par."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data and Package Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer, TransformedTargetRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('../data/train_clean.csv')\n",
    "df_test = pd.read_csv('../data/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train_Test_Split and Separating Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating numeric and categorical columns to be used in one-hot encoding\n",
    "numeric = [col for col in df_train._get_numeric_data().columns if not col in ['id','sale_price']]\n",
    "categorical = [col for col in df_train.columns if col not in numeric and col not in ['id','sale_price']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_train.drop(columns=['id','sale_price'])\n",
    "y = df_train['sale_price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    random_state=13\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines and Model Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling and Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column Transformer made to scale input data and encode categorical data.\n",
    "# Can apply to all models. This separates numeric and categorical variables for model fit. In theory, only needs to be instantiated once.\n",
    "\n",
    "ctx = ColumnTransformer(\n",
    "    [\n",
    "    ('ss', StandardScaler(), numeric),\n",
    "    ('ohe', OneHotEncoder(\n",
    "        handle_unknown='ignore',\n",
    "        drop='first'),\n",
    "        # Drops the first categorical variable, and all coefficients are to be interpreted in relation to the dropped variable\n",
    "        # For 'neighborhood': dropped variable is 'Sawyer'\n",
    "        # For 'home_type': dropped variable is 'Newer 2 Story' (2 story home built in 1946 or newer)\n",
    "        categorical)\n",
    "    ]\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiating Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression Pipeline\n",
    "lr_pipe = Pipeline([\n",
    "    ('ctx', ctx),\n",
    "    ('lr', TransformedTargetRegressor(LinearRegression(), func=np.log, inverse_func=np.exp))\n",
    "])\n",
    "\n",
    "# Ridge Regression Pipeline\n",
    "rg_pipe = Pipeline([\n",
    "    ('ctx', ctx),\n",
    "    ('rg', TransformedTargetRegressor(Ridge(), func=np.log, inverse_func=np.exp))\n",
    "])\n",
    "\n",
    "# Lasso Regression Pipeline\n",
    "lasso_pipe = Pipeline([\n",
    "    ('ctx', ctx),\n",
    "    ('lasso', TransformedTargetRegressor(Lasso(), func=np.log, inverse_func=np.exp))\n",
    "])\n",
    "\n",
    "# ElasticNet Pipeline\n",
    "enet_pipe = Pipeline([\n",
    "    ('ctx', ctx),\n",
    "    ('enet', TransformedTargetRegressor(ElasticNet(), func=np.log, inverse_func=np.exp))\n",
    "])\n",
    "\n",
    "# KNeighborsRegressor Pipeline\n",
    "knn_pipe = Pipeline([\n",
    "    ('ctx', ctx),\n",
    "    ('knn', TransformedTargetRegressor(KNeighborsRegressor(), func=np.log, inverse_func=np.exp))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model RMSE Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate null predictions to compare our models to\n",
    "y_preds_null = np.full_like(y, y.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('ctx',\n",
       "                 ColumnTransformer(transformers=[('ss', StandardScaler(),\n",
       "                                                  ['home_quality', 'home_sqft',\n",
       "                                                   'garage_cars', 'home_age',\n",
       "                                                   'yr_remod', 'full_bath',\n",
       "                                                   'masonry_veneer_area',\n",
       "                                                   'total_rooms_above_ground']),\n",
       "                                                 ('ohe',\n",
       "                                                  OneHotEncoder(drop='first',\n",
       "                                                                handle_unknown='ignore'),\n",
       "                                                  ['neighborhood',\n",
       "                                                   'home_type'])])),\n",
       "                ('knn',\n",
       "                 TransformedTargetRegressor(func=<ufunc 'log'>,\n",
       "                                            inverse_func=<ufunc 'exp'>,\n",
       "                                            regressor=KNeighborsRegressor()))])"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting models\n",
    "lr_pipe.fit(X_train, y_train)\n",
    "rg_pipe.fit(X_train, y_train)\n",
    "lasso_pipe.fit(X_train, y_train)\n",
    "enet_pipe.fit(X_train, y_train)\n",
    "knn_pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are comparing out model RMSEs to the Null RMSE to see if the error of our models are acceptable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null RMSE: 65897.4\n",
      "\n",
      "Linear Regression RMSE: 20387.94\n",
      "Ridge RMSE: 20418.25\n",
      "Lasso RMSE: 66625.75\n",
      "ElasticNet RMSE: 66625.75\n",
      "KNearestRegressor RMSE: 20034.5\n"
     ]
    }
   ],
   "source": [
    "print('Null RMSE:', round(np.sqrt(mean_squared_error(y, y_preds_null)), 2))\n",
    "print('')\n",
    "print('Linear Regression RMSE:', round(np.sqrt(mean_squared_error(y_train, lr_pipe.predict(X_train))), 2))\n",
    "print('Ridge RMSE:', round(np.sqrt(mean_squared_error(y_train, rg_pipe.predict(X_train))), 2))\n",
    "print('Lasso RMSE:', round(np.sqrt(mean_squared_error(y_train, lasso_pipe.predict(X_train))), 2))\n",
    "print('ElasticNet RMSE:', round(np.sqrt(mean_squared_error(y_train, enet_pipe.predict(X_train))), 2))\n",
    "print('KNearestRegressor RMSE:', round(np.sqrt(mean_squared_error(y_train, knn_pipe.predict(X_train))), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the moment, judging solely by RMSE, the only usable models are Linear Regression, Ridge, and KNearestRegressor. The best model based on RMSE is KNearestRegressor and the worst models are ElasticNet and Lasso. We can also use the disparity between training and testing scores for these models to measure their effectiveness in generating predictions and generating insights on individual feature effect on sale price."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model R2 / Accuracy Score Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression Training Score: 0.893\n",
      "Linear Regression Validation Score: 0.857\n",
      "\n",
      "Ridge Training Score: 0.893\n",
      "Ridge Validation Score: 0.863\n",
      "\n",
      "Lasso Training Score: -0.037\n",
      "Lasso Validation Score: -0.047\n",
      "\n",
      "ElasticNet Training Score: -0.037\n",
      "ElasticNet Validation Score: -0.047\n",
      "\n",
      "KNearestRegressor Training Score: 0.849\n",
      "KNearestRegressor Validation Score: 0.782\n"
     ]
    }
   ],
   "source": [
    "print('Linear Regression Training Score:', round(cross_val_score(lr_pipe, X_train, y_train).mean(), 3))\n",
    "print('Linear Regression Validation Score:', round(cross_val_score(lr_pipe, X_val, y_val).mean(), 3))\n",
    "print(\"\")\n",
    "print('Ridge Training Score:', round(cross_val_score(rg_pipe, X_train, y_train).mean(), 3))\n",
    "print('Ridge Validation Score:', round(cross_val_score(rg_pipe, X_val, y_val).mean(), 3))\n",
    "print(\"\")\n",
    "print('Lasso Training Score:', round(cross_val_score(lasso_pipe, X_train, y_train).mean(), 3))\n",
    "print('Lasso Validation Score:', round(cross_val_score(lasso_pipe, X_val, y_val).mean(), 3))\n",
    "print(\"\")\n",
    "print('ElasticNet Training Score:', round(cross_val_score(enet_pipe, X_train, y_train).mean(), 3))\n",
    "print('ElasticNet Validation Score:', round(cross_val_score(enet_pipe, X_val, y_val).mean(), 3))\n",
    "print(\"\")\n",
    "print('KNearestRegressor Training Score:', round(cross_val_score(knn_pipe, X_train, y_train).mean(), 3))\n",
    "print('KNearestRegressor Validation Score:', round(cross_val_score(knn_pipe, X_val, y_val).mean(), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the first 4 scores, these scores (R^2) are an evaluator as to how much of the data's variance they can account for. For KNearestRegressor, its score is accuracy. Based on these scores, every model is within a 3% threshold of underfit/overfit, but are slightly overfit. Our models are slightly worse at predicting with new data than predicting y_train. This is likely due to the large amount of features, and small amount of observations. The most overfit of all of the models is KNearestRegressor. Based solely on scores, the best model is Ridge and the worst model is ElasticNet.\n",
    "\n",
    "We can use the coefficients from Ridge to interpret the effects that our features have on home sale price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Features</th>\n",
       "      <th>Coefficient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ss__home_sqft</td>\n",
       "      <td>0.173751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ohe__neighborhood_Crawfor</td>\n",
       "      <td>0.163102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ohe__neighborhood_ClearCr</td>\n",
       "      <td>0.132807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>ohe__neighborhood_Veenker</td>\n",
       "      <td>0.120682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>ohe__home_type_Split Foyer</td>\n",
       "      <td>0.114954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>ohe__home_type_2 Story PUD</td>\n",
       "      <td>-0.114078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ohe__neighborhood_IDOTRR</td>\n",
       "      <td>-0.107370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ss__home_quality</td>\n",
       "      <td>0.102906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ohe__neighborhood_BrDale</td>\n",
       "      <td>-0.084809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>ohe__home_type_Duplex</td>\n",
       "      <td>-0.079169</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Features  Coefficient\n",
       "1                ss__home_sqft     0.173751\n",
       "13   ohe__neighborhood_Crawfor     0.163102\n",
       "11   ohe__neighborhood_ClearCr     0.132807\n",
       "33   ohe__neighborhood_Veenker     0.120682\n",
       "43  ohe__home_type_Split Foyer     0.114954\n",
       "37  ohe__home_type_2 Story PUD    -0.114078\n",
       "17    ohe__neighborhood_IDOTRR    -0.107370\n",
       "0             ss__home_quality     0.102906\n",
       "9     ohe__neighborhood_BrDale    -0.084809\n",
       "38       ohe__home_type_Duplex    -0.079169"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {\n",
    "    'Features' : ctx.get_feature_names_out(),\n",
    "    'Coefficient' : np.exp(rg_pipe.named_steps.rg.regressor_.coef_) - 1 # Need to do a .exp() and -1 because I used the log of my y variable to make the coefficients interpretable.\n",
    "}\n",
    "\n",
    "coefficients = pd.DataFrame(data).sort_values(\n",
    "                by=\"Coefficient\",\n",
    "                ascending=False, # largest first\n",
    "                key=abs) # Sorted by absolute value\n",
    "\n",
    "coefficients.head(10) # top 10 coefficient values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we used the log of our y to normalize sale price, the way that we interpret our coefficients is different from how it usually is with regression-based models. For every 1 unit change in home square footage, assuming all else constant, home price increases by 17%. To use a categorical variable as an example, if two homes were in different neighborhoods, assuming all else constant, the price of a home in Crawford would be worth 15% more than the price of a home in Sawyer. It is apparent that the strongest variables in our feature set are home square footage, home type (originally MS Subclass), neighborhood, and home quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of Polynomial Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new instance of ctx to include polynomial features and only apply polynomial features and standardscaler\n",
    "pfs = Pipeline([\n",
    "    ('pf', PolynomialFeatures(\n",
    "        degree=2,\n",
    "        interaction_only=True,\n",
    "        include_bias=False\n",
    "    )),\n",
    "    ('sc', StandardScaler()),\n",
    "])\n",
    "\n",
    "# Separating polynomial features and standardscaler from column transformer so that standardscaler can scale polynomial features and all numeric features\n",
    "\n",
    "ctx_poly = ColumnTransformer([\n",
    "    ('pfs', pfs, numeric),\n",
    "    ('ohe', OneHotEncoder(\n",
    "        handle_unknown='ignore',\n",
    "        drop='first'\n",
    "        # Drops the first categorical variable, and all coefficients are to be interpreted in relation to the dropped variable\n",
    "        # For 'neighborhood': dropped variable is 'Sawyer'\n",
    "        # For 'home_type': dropped variable is 'Newer 2 Story' (2 story home built in 1946 or newer)\n",
    "    ), categorical)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not applying TransformedTargetRegressor here, after some testing it doesn't seem to affect RMSE or R^2 scores when applied with polynomial features.\n",
    "\n",
    "# Linear Regression Pipeline\n",
    "lr_pipe_poly = Pipeline([\n",
    "    ('ctx', ctx_poly),\n",
    "    ('lr', LinearRegression())\n",
    "])\n",
    "\n",
    "# Ridge Regression Pipeline\n",
    "rg_pipe_poly = Pipeline([\n",
    "    ('ctx', ctx_poly),\n",
    "    ('rg', Ridge())\n",
    "])\n",
    "\n",
    "# Lasso Regression Pipeline\n",
    "lasso_pipe_poly = Pipeline([\n",
    "    ('ctx', ctx_poly),\n",
    "    ('lasso', Lasso())\n",
    "])\n",
    "\n",
    "# ElasticNet Pipeline\n",
    "enet_pipe_poly = Pipeline([\n",
    "    ('ctx', ctx_poly),\n",
    "    ('enet', ElasticNet())\n",
    "])\n",
    "\n",
    "# KNeighborsRegressor Pipeline\n",
    "knn_pipe_poly = Pipeline([\n",
    "    ('ctx', ctx_poly),\n",
    "    ('knn', KNeighborsRegressor())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('ctx',\n",
       "                 ColumnTransformer(transformers=[('pfs',\n",
       "                                                  Pipeline(steps=[('pf',\n",
       "                                                                   PolynomialFeatures(include_bias=False,\n",
       "                                                                                      interaction_only=True)),\n",
       "                                                                  ('sc',\n",
       "                                                                   StandardScaler())]),\n",
       "                                                  ['home_quality', 'home_sqft',\n",
       "                                                   'garage_cars', 'home_age',\n",
       "                                                   'yr_remod', 'full_bath',\n",
       "                                                   'masonry_veneer_area',\n",
       "                                                   'total_rooms_above_ground']),\n",
       "                                                 ('ohe',\n",
       "                                                  OneHotEncoder(drop='first',\n",
       "                                                                handle_unknown='ignore'),\n",
       "                                                  ['neighborhood',\n",
       "                                                   'home_type'])])),\n",
       "                ('knn', KNeighborsRegressor())])"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting models again, this time including polynomial features\n",
    "lr_pipe_poly.fit(X_train, y_train)\n",
    "rg_pipe_poly.fit(X_train, y_train)\n",
    "lasso_pipe_poly.fit(X_train, y_train)\n",
    "enet_pipe_poly.fit(X_train, y_train)\n",
    "knn_pipe_poly.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model RMSE Comparison - Polynomials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null RMSE: 65897.4\n",
      "\n",
      "Linear Regression RMSE: 19839.22\n",
      "Ridge RMSE: 20000.34\n",
      "Lasso RMSE: 19992.44\n",
      "ElasticNet RMSE: 24301.74\n",
      "KNearestRegressor RMSE: 20016.61\n"
     ]
    }
   ],
   "source": [
    "print('Null RMSE:', round(np.sqrt(mean_squared_error(y, y_preds_null)), 2))\n",
    "print('')\n",
    "print('Linear Regression RMSE:', round(np.sqrt(mean_squared_error(y_train, lr_pipe_poly.predict(X_train))), 2))\n",
    "print('Ridge RMSE:', round(np.sqrt(mean_squared_error(y_train, rg_pipe_poly.predict(X_train))), 2))\n",
    "print('Lasso RMSE:', round(np.sqrt(mean_squared_error(y_train, lasso_pipe_poly.predict(X_train))), 2))\n",
    "print('ElasticNet RMSE:', round(np.sqrt(mean_squared_error(y_train, enet_pipe_poly.predict(X_train))), 2))\n",
    "print('KNearestRegressor RMSE:', round(np.sqrt(mean_squared_error(y_train, knn_pipe_poly.predict(X_train))), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the moment, judging solely by RMSE, all of our models are still valid and can identify relationships and produce useful predictions from polynomial data. The best model is LinearRegression, and the worst model is ElasticNet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model R^2 / Accuracy Score Comparison - Polynomials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression Training Score: 0.893\n",
      "Linear Regression Validation Score: 0.832\n",
      "\n",
      "Ridge Training Score: 0.894\n",
      "Ridge Validation Score: 0.847\n",
      "\n",
      "Lasso Training Score: 0.893\n",
      "Lasso Validation Score: 0.835\n",
      "\n",
      "ElasticNet Training Score: 0.859\n",
      "ElasticNet Validation Score: 0.827\n",
      "\n",
      "KNearestRegressor Training Score: 0.848\n",
      "KNearestRegressor Validation Score: 0.776\n"
     ]
    }
   ],
   "source": [
    "print('Linear Regression Training Score:', round(cross_val_score(lr_pipe_poly, X_train, y_train).mean(), 3))\n",
    "print('Linear Regression Validation Score:', round(cross_val_score(lr_pipe_poly, X_val, y_val).mean(), 3))\n",
    "print(\"\")\n",
    "print('Ridge Training Score:', round(cross_val_score(rg_pipe_poly, X_train, y_train).mean(), 3))\n",
    "print('Ridge Validation Score:', round(cross_val_score(rg_pipe_poly, X_val, y_val).mean(), 3))\n",
    "print(\"\")\n",
    "print('Lasso Training Score:', round(cross_val_score(lasso_pipe_poly, X_train, y_train).mean(), 3))\n",
    "print('Lasso Validation Score:', round(cross_val_score(lasso_pipe_poly, X_val, y_val).mean(), 3))\n",
    "print(\"\")\n",
    "print('ElasticNet Training Score:', round(cross_val_score(enet_pipe_poly, X_train, y_train).mean(), 3))\n",
    "print('ElasticNet Validation Score:', round(cross_val_score(enet_pipe_poly, X_val, y_val).mean(), 3))\n",
    "print(\"\")\n",
    "print('KNearestRegressor Training Score:', round(cross_val_score(knn_pipe_poly, X_train, y_train).mean(), 3))\n",
    "print('KNearestRegressor Validation Score:', round(cross_val_score(knn_pipe_poly, X_val, y_val).mean(), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on these scores, every model is still slightly overfit, potentially more overfit. Our models are slightly worse at predicting with new data than predicting y_train. This is likely due to the large amount of features, and small amount of observations. Based solely on scores, the best model is Ridge and the worst model is KNearestRegressor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GridSearchCV for Optimization of Ridge - Original"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the non-Polynomial Ridge model RMSE was very close to the Polynomial scores, for the sake of coefficient interpretability, I am going to use the original Ridge model as my final model for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "rg_params = {\n",
    "    'rg__regressor__alpha' : [1, 2, 3, 4, 5],\n",
    "    'rg__regressor__random_state' : [1, 2, 3, 4, 5],\n",
    "    'rg__regressor__solver' : ['auto', 'sag', 'lbfgs']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = GridSearchCV(\n",
    "    rg_pipe,\n",
    "    rg_params,\n",
    "    cv = 5,\n",
    "    scoring='neg_mean_squared_error'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('ctx',\n",
       "                                        ColumnTransformer(transformers=[('ss',\n",
       "                                                                         StandardScaler(),\n",
       "                                                                         ['home_quality',\n",
       "                                                                          'home_sqft',\n",
       "                                                                          'garage_cars',\n",
       "                                                                          'home_age',\n",
       "                                                                          'yr_remod',\n",
       "                                                                          'full_bath',\n",
       "                                                                          'masonry_veneer_area',\n",
       "                                                                          'total_rooms_above_ground']),\n",
       "                                                                        ('ohe',\n",
       "                                                                         OneHotEncoder(drop='first',\n",
       "                                                                                       handle_unknown='ignore'),\n",
       "                                                                         ['neighborhood',\n",
       "                                                                          'home_type'])])),\n",
       "                                       ('rg',\n",
       "                                        TransformedTargetRegressor(func=<ufunc 'log'>,\n",
       "                                                                   inverse_func=<ufunc 'exp'>,\n",
       "                                                                   regressor=Ridge()))]),\n",
       "             param_grid={'rg__regressor__alpha': [1, 2, 3, 4, 5],\n",
       "                         'rg__regressor__random_state': [1, 2, 3, 4, 5],\n",
       "                         'rg__regressor__solver': ['auto', 'sag', 'lbfgs']},\n",
       "             scoring='neg_mean_squared_error')"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21331.47560514137"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(-gs.best_score_) # Best RMSE score of GridSearch optimized Ridge regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rg__regressor__alpha': 4,\n",
       " 'rg__regressor__random_state': 3,\n",
       " 'rg__regressor__solver': 'sag'}"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_params_ # GridSearch returned best parameters for Ridge regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After some testing, I've found that the best Ridge regression parameters are the default ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Model Fitting - X_Val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Ridge Regression Pipeline with default parameters\n",
    "rg_pipe = Pipeline([\n",
    "    ('ctx', ctx),\n",
    "    ('rg', TransformedTargetRegressor(Ridge(), func=np.log, inverse_func=np.exp))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('ctx',\n",
       "                 ColumnTransformer(transformers=[('ss', StandardScaler(),\n",
       "                                                  ['home_quality', 'home_sqft',\n",
       "                                                   'garage_cars', 'home_age',\n",
       "                                                   'yr_remod', 'full_bath',\n",
       "                                                   'masonry_veneer_area',\n",
       "                                                   'total_rooms_above_ground']),\n",
       "                                                 ('ohe',\n",
       "                                                  OneHotEncoder(drop='first',\n",
       "                                                                handle_unknown='ignore'),\n",
       "                                                  ['neighborhood',\n",
       "                                                   'home_type'])])),\n",
       "                ('rg',\n",
       "                 TransformedTargetRegressor(func=<ufunc 'log'>,\n",
       "                                            inverse_func=<ufunc 'exp'>,\n",
       "                                            regressor=Ridge()))])"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rg_pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Ridge RMSE (Train): 20000.34\n",
      "Final Ridge RMSE (Test): 23380.07\n"
     ]
    }
   ],
   "source": [
    "print('Final Ridge RMSE (Train):', round(np.sqrt(mean_squared_error(y_train, rg_pipe_poly.predict(X_train))), 2))\n",
    "print('Final Ridge RMSE (Test):', round(np.sqrt(mean_squared_error(y_val, rg_pipe_poly.predict(X_val))), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge Training Score: 0.894\n",
      "Ridge Validation Score: 0.847\n"
     ]
    }
   ],
   "source": [
    "print('Ridge Training Score:', round(cross_val_score(rg_pipe_poly, X_train, y_train).mean(), 3))\n",
    "print('Ridge Validation Score:', round(cross_val_score(rg_pipe_poly, X_val, y_val).mean(), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Ridge Model on Official Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning test data so that the model properly recognizes variables\n",
    "\n",
    "def clean_df_test(df):\n",
    "    dict_home_type = { # Creating a dictionary that correlates to the data dictionary and will replace numerics with string values in the 'home_type' / 'MS SubClass' column.\n",
    "    20 : 'Newer 1 Story', \n",
    "    30 : 'Older 1 Story', \n",
    "    40 : '1 Story Finished Attic',\n",
    "    45 : '1.5 Story Unfinished',\n",
    "    50 : '1.5 Story Finished',\n",
    "    60 : 'Newer 2 Story',\n",
    "    70 : 'Older 2 Story',\n",
    "    75 : '2.5 Story',\n",
    "    80 : 'Split or Multi-Level',\n",
    "    85 : 'Split Foyer',\n",
    "    90 : 'Duplex',\n",
    "    120 : '1 Story PUD',\n",
    "    150 : '1.5 Story PUD',\n",
    "    160 : '2 Story PUD',\n",
    "    180 : 'PUD Multilevel',\n",
    "    190 : '2 Family Conversion'\n",
    "}\n",
    "    \n",
    "    df['home_sqft'] = df['Gr Liv Area'] + df['Garage Area'] + df['Total Bsmt SF'] # Creating a new SqFt variable that captures total living area sqft + garage sqft + basement sqft\n",
    "    df['home_age'] = df['Yr Sold'] - df['Year Built']\n",
    "    df = df[df['home_age'] >= 0] # removing any rows where home_age might be negative, as that isn't possible and indicates an error in the original data\n",
    "    df = df[[ # Filtering dataset by most important variables needed\n",
    "    'Id', # ID of the home. Need this to complete the task.\n",
    "    'Overall Qual', # Quality of the exterior of the home, rated 1-10.\n",
    "    'home_sqft', # Sum SqFt of Garage, Living Areas, and Basement\n",
    "    'Garage Cars', # Garage size by how many cars could fit (e.g. 2-car garage)\n",
    "    'home_age',\n",
    "    'Year Remod/Add', # Year remodelled. If not remodelled, same year as Year Built\n",
    "    'Full Bath', # Number of full baths on the property\n",
    "    'Mas Vnr Area', # Amount of masonry veneer - exterior decoration (e.g. brick, stone, etc.)\n",
    "    'TotRms AbvGrd', # Total rooms above the basement.\n",
    "    'Neighborhood',\n",
    "    'MS SubClass']]\n",
    "\n",
    "    df.replace({'MS SubClass' : dict_home_type}, inplace=True) # replacing numeric keys in series with string values in dict_home_type\n",
    "\n",
    "    df = df.rename(columns={ # renaming columns to snake case\n",
    "    'Id' : 'id',\n",
    "    'Overall Qual' : 'home_quality',\n",
    "    'Garage Cars' : 'garage_cars',\n",
    "    'Year Remod/Add' : 'yr_remod',\n",
    "    'Full Bath' : 'full_bath',\n",
    "    'Mas Vnr Area' : 'masonry_veneer_area',\n",
    "    'TotRms AbvGrd' : 'total_rooms_above_ground',\n",
    "    'Neighborhood' : 'neighborhood',\n",
    "    'MS SubClass' : 'home_type'\n",
    "    })\n",
    "\n",
    "    df = df.fillna(0)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "df_test = clean_df_test(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_id = df_test['id'] # Storing the id column\n",
    "df_test = df_test.drop(columns=['id']) # Dropping the id column in df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = rg_pipe.predict(df_test) # Final prediction\n",
    "preds = pd.DataFrame(preds, columns = ['SalePrice']) # Create submission dataframe\n",
    "preds.insert(loc = 0, column = 'Id', value = test_id) # Insert ID column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds.to_csv('../submissions/ridge_3.csv', index=False) # Final Kaggle RMSE Score: 23638"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final Ridge model can account for approximately 84% of variance in new, unseen data, and our RMSE on unseen data is 23638. This means that there is an element of risk - the positive and negative range of potential error that the model could predict is up to $23,638 when our model is trying to predict home price. At a higher scale, and especially in today's housing market, this error is minimal, especially beyond the $400,000 range. Also, our model cannot account for 16% of variance in the data. Comparing this to the null RMSE (65897), using our model to predict home price or interpret which variables have a measured effect is going to be much more effective than simply guessing by using the mean home price."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "04dcdbdaba10c3e859b809de9f739430c63b86655a1eec790cb299d9c56247cc"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit (system)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
